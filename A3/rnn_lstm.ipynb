{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    !pip3 install torch matplotlib torchmetrics scikit-image segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence # for padding data\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import wandb\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Support?\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using the GPU\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"Using the CPU\")\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Generate a dataset of sequences containing characters 'a', 'b', and 'c' in order\n",
    "N = 1000 # Size of dataset (number of sequences)\n",
    "dataset = []\n",
    "\n",
    "for n in range(N):\n",
    "    if n % 4 == 0: # 25% of the time add actual member of formal language family\n",
    "        length = random.randint(1, 6)\n",
    "        sequence = 'a' * length + 'b' * length + 'c' * length\n",
    "        dataset.append(sequence)\n",
    "    else:\n",
    "        length = random.randint(3, 17)  # Random sequence length between 3 and (20-3 = 17). 3 is the smallest possible length for a sequence to be in the language\n",
    "        counts = [1, 1, 1] # Initialize counts for 'a', 'b', and 'c'. Will have at least one of each letter.\n",
    "\n",
    "        # Distribute the length among a, b, and c\n",
    "        for i in range(length):\n",
    "            counts[random.randint(0, 2)] += 1\n",
    "\n",
    "        # Ensure alphabetical order and create the sequence\n",
    "        sequence = 'a' * counts[0] + 'b' * counts[1] + 'c' * counts[2]\n",
    "        dataset.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print max length of dataset\n",
    "max_len = max([len(s) for s in dataset])\n",
    "min_len = min([len(s) for s in dataset])\n",
    "print(min_len, max_len) # Should be 3, 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for labels and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels\n",
    "def get_labels(data):\n",
    "    y = torch.zeros(len(data))\n",
    "    for i, sequence in enumerate(data):\n",
    "        if sequence == 'a'*(len(sequence)//3) + 'b'*(len(sequence)//3) + 'c'*(len(sequence)//3):\n",
    "            y[i] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a one-hot encoding of the sequences and a labels tensor\n",
    "def one_hot_encode(sequence):\n",
    "    encoded = torch.zeros(len(sequence), 3)\n",
    "    for i, char in enumerate(sequence):\n",
    "        encoded[i, 'abc'.index(char)] = 1\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = [one_hot_encode(sequence) for sequence in dataset]\n",
    "print(encoded_dataset[0], \"\\n\", dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sequences\n",
    "padded_dataset = pad_sequence(encoded_dataset, batch_first=True)\n",
    "print(padded_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make training data\n",
    "train_size = int(0.8*N) # 80/20 train/test split\n",
    "test_size = N - train_size\n",
    "\n",
    "train_data = padded_dataset[:train_size]\n",
    "test_data = padded_dataset[train_size:]\n",
    "y_train = get_labels(dataset[:train_size])\n",
    "y_test = get_labels(dataset[train_size:])\n",
    "print(train_data.shape, test_data.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(list(zip(train_data, y_train)), batch_size=train_data.size(0), shuffle=True) # HVAD SKAL BATCH SIZE VÃ†RE??\n",
    "test_loader = DataLoader(list(zip(test_data, y_test)), batch_size=test_data.size(0), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test train_loader\n",
    "for data, labels in train_loader:\n",
    "    print(data.shape, labels.shape)\n",
    "    print(data[0], labels[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # x -> (batch_length, seq_length, input_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        h0 = torch.zeros(self.num_layers, input.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.rnn(input, h0)\n",
    "        # out -> (batch_size, seq_length, hidden_size)\n",
    "        # reshape to get last output\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training (Change; make function and prep sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hyperparams\n",
    "input_size =  3 # 'a' 'b' 'c'\n",
    "num_classes = 1 # binary classification\n",
    "hidden_size = 128 # hyperparameter; can be tuned\n",
    "num_layers = 1 # hyperparameter; can be tuned\n",
    "\n",
    "rnn = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "learning_rate = 0.005 # hyperparameter; can be tuned\n",
    "optimizer = optim.SGD(rnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    for data, labels in train_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = rnn(data)\n",
    "        loss = criterion(outputs.squeeze(), labels) # BCEWithLogitsLoss expects 1D input, output from RNN is 2D\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (Change; Make function and determine validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "rnn.eval()\n",
    "\n",
    "# Initialize lists to store predictions and actual labels\n",
    "train_predictions = []\n",
    "test_predictions = []\n",
    "train_actuals = []\n",
    "test_actuals = []\n",
    "\n",
    "# Iterate over the test data\n",
    "for inputs, labels in test_loader:\n",
    "    # Move inputs and labels to the same device as your model\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Get the model's predictions\n",
    "    outputs = rnn(inputs)\n",
    "\n",
    "    # Convert the outputs to binary predictions (0 or 1)\n",
    "    pred = torch.round(torch.sigmoid(outputs))\n",
    "\n",
    "    # Store the predictions and actual labels\n",
    "    test_predictions.extend(pred.tolist())\n",
    "    test_actuals.extend(labels.tolist())\n",
    "\n",
    "# Iterate over the training data\n",
    "for inputs, labels in train_loader:\n",
    "    # Move inputs and labels to the same device as your model\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Get the model's predictions\n",
    "    outputs = rnn(inputs)\n",
    "\n",
    "    # Convert the outputs to binary predictions (0 or 1)\n",
    "    pred = torch.round(torch.sigmoid(outputs))\n",
    "\n",
    "    # Store the predictions and actual labels\n",
    "    train_predictions.extend(pred.tolist())\n",
    "    train_actuals.extend(labels.tolist())\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "train_accuracy = accuracy_score(train_actuals, train_predictions)\n",
    "test_accuracy = accuracy_score(test_actuals, test_predictions)\n",
    "f1 = f1_score(test_actuals, test_predictions)\n",
    "\n",
    "print(f'Training Accuracy: {test_accuracy*100:.2f}%')\n",
    "print(f'Test Accuracy: {test_accuracy*100:.2f}%')\n",
    "print(f'F1 Score: {f1:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
