{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    !pip3 install torch matplotlib torchmetrics scikit-image segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence # for padding data\n",
    "\n",
    "import pandas as pd # for making csv file\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import wandb\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Support?\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using the GPU\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"Using the CPU\")\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Generate a dataset of sequences containing characters 'a', 'b', and 'c' in order\n",
    "N = 10000 # Size of dataset (number of sequences)\n",
    "dataset = []\n",
    "\n",
    "for n in range(N):\n",
    "    if n % 4 == 0: # 25% of the time add actual member of formal language family\n",
    "        length = random.randint(1, 6)\n",
    "        sequence = 'a' * length + 'b' * length + 'c' * length\n",
    "        dataset.append(sequence)\n",
    "    else:\n",
    "        length = random.randint(3, 17)  # Random sequence length between 3 and (20-3 = 17). 3 is the smallest possible length for a sequence to be in the language\n",
    "        counts = [1, 1, 1] # Initialize counts for 'a', 'b', and 'c'. Will have at least one of each letter.\n",
    "\n",
    "        # Distribute the length among a, b, and c\n",
    "        for i in range(length):\n",
    "            counts[random.randint(0, 2)] += 1\n",
    "\n",
    "        # Ensure alphabetical order and create the sequence\n",
    "        sequence = 'a' * counts[0] + 'b' * counts[1] + 'c' * counts[2]\n",
    "        dataset.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print max length of dataset\n",
    "max_len = max([len(s) for s in dataset])\n",
    "min_len = min([len(s) for s in dataset])\n",
    "print(min_len, max_len) # Should be 3, 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for labels and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels\n",
    "def get_labels(data):\n",
    "    y = torch.zeros(len(data))\n",
    "    for i, sequence in enumerate(data):\n",
    "        if sequence == 'a'*(len(sequence)//3) + 'b'*(len(sequence)//3) + 'c'*(len(sequence)//3):\n",
    "            y[i] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a one-hot encoding of the sequences and a labels tensor\n",
    "def one_hot_encode(sequence):\n",
    "    encoded = torch.zeros(len(sequence), 3)\n",
    "    for i, char in enumerate(sequence):\n",
    "        encoded[i, 'abc'.index(char)] = 1\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = [one_hot_encode(sequence) for sequence in dataset]\n",
    "print(encoded_dataset[0], \"\\n\", dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sequences\n",
    "padded_dataset = pad_sequence(encoded_dataset, batch_first=True)\n",
    "print(padded_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make training data\n",
    "train_size = int(0.8*N) # 80/20 train/test split\n",
    "test_size = N - train_size\n",
    "\n",
    "train_data = padded_dataset[:train_size]\n",
    "test_data = padded_dataset[train_size:]\n",
    "y_train = get_labels(dataset[:train_size])\n",
    "y_test = get_labels(dataset[train_size:])\n",
    "print(train_data.shape, test_data.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.sum()/len(y_train) # % of sequences that are in the language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(list(zip(train_data, y_train)), batch_size=128, shuffle=True) # HVAD SKAL BATCH SIZE VÃ†RE??\n",
    "test_loader = DataLoader(list(zip(test_data, y_test)), batch_size=test_data.size(0), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test train_loader\n",
    "for data, labels in train_loader:\n",
    "    print(data.shape, labels.shape)\n",
    "    print(data[0], labels[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # x -> (batch_length, seq_length, input_size/vocab_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out, _ = self.rnn(input)\n",
    "        # out -> (batch_size, seq_length, hidden_size)\n",
    "        # reshape to get last output\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # x -> (batch_length, seq_length, input_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out, _ = self.lstm(input)\n",
    "        # out -> (batch_size, seq_length, hidden_size)\n",
    "        # reshape to get last output\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "input_size =  3 # 'a' 'b' 'c'\n",
    "num_classes = 1 # binary classification\n",
    "hidden_size = 50 # hyperparameter; can be tuned\n",
    "num_layers = 1 # hyperparameter; can be tuned\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss() # \"hyperparameter\" (maybe BCE without LogitsLoss is better?)\n",
    "learning_rate = 0.005 # hyperparameter; can be tuned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "optimizer = optim.SGD(rnn.parameters(), lr=learning_rate) # SGD (vanishing gradiant midigation)\n",
    "\n",
    "# Training loop RNN\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    for data, labels in train_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = rnn(data)\n",
    "        loss = criterion(outputs.squeeze(), labels) # BCEWithLogitsLoss expects 1D input, output from RNN is 2D\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=learning_rate) # Adam (no vanishing gradient problem)\n",
    "\n",
    "# Training loop LSTM\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for data, labels in train_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = lstm(data)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make dataset with labels to csv\n",
    "# df = pd.DataFrame(dataset)\n",
    "# df['label'] = get_labels(dataset)\n",
    "# df.to_csv('formal_language.csv', index=False)\n",
    "\n",
    "# # Instantiate a WandB run\n",
    "# wandb.login()\n",
    "# run = wandb.init(project=\"formal_language_rnn_lstm\")\n",
    "\n",
    "# # Create an artifact for data\n",
    "# artifact = wandb.Artifact(\"formal_language_data\", type=\"dataset\") \n",
    "# artifact.add_file(\"formal_language.csv\") \n",
    "# run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sweeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep config\n",
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"loss\", \"goal\": \"minimize\"},\n",
    "    \"parameters\": {\n",
    "        \"model\": { \"values\": [\"RNN\", \"LSTM\"] },\n",
    "        \"epochs\": {\"values\": [200, 500] },    \n",
    "        \"optimizer\": { \"values\": [\"SGD\", \"Adam\"] },\n",
    "        \"hidden_size\": {\n",
    "            \"values\": [2, 20, 50]\n",
    "        },\n",
    "        \"num_layers\": {\n",
    "            \"values\": [1, 2]\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.001, 0.01]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    with wandb.init(project=\"formal_language_rnn_lstm\", config=config):\n",
    "        config = wandb.config\n",
    "\n",
    "        # Get hyperparameters\n",
    "        hidden_size = config.hidden_size\n",
    "        num_layers = config.num_layers\n",
    "        learning_rate = config.learning_rate\n",
    "        num_epochs = config.epochs\n",
    "\n",
    "        # Input size and number of classes\n",
    "        num_classes = 1 # binary classification\n",
    "        input_size = 3 # 'a' 'b' 'c'\n",
    "\n",
    "        # Set criterion\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Get model\n",
    "        if config.model == \"RNN\":\n",
    "            model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "        else:\n",
    "            model = LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "        \n",
    "        # Get optimizer\n",
    "        if config.optimizer == \"SGD\":\n",
    "            optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "        else:\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        wandb.watch(model, criterion, log=\"all\")\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            for data, labels in train_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                wandb.log({\"epoch\": epoch+1, \"loss\": loss.item()})\n",
    "            if (epoch+1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "        print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sweep_id\n",
    "# sweep_id = wandb.sweep(sweep_config, project=\"formal_language_rnn_lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the sweep\n",
    "# wandb.agent(sweep_id, function=train, count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_evaluation(model, loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data, label in loader:\n",
    "            data, label = data.to(device), label.to(device)\n",
    "\n",
    "            # Get the model's predictions\n",
    "            output = model(data)\n",
    "            pred = torch.round(torch.sigmoid(output)) # Sigmoid to get probabilities, round to get binary predictions\n",
    "\n",
    "            y_true.extend(label.tolist())\n",
    "            y_pred.extend(pred.tolist())\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report F1 Score (and accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_accuracy, rnn_f1 = report_evaluation(rnn, test_loader)\n",
    "lstm_accuracy, lstm_f1 = report_evaluation(lstm, test_loader)\n",
    "\n",
    "\n",
    "print(f'RNN Test Accuracy: {rnn_accuracy*100:.2f}%')\n",
    "print(f'RNN F1 Score: {rnn_f1:.2f}')\n",
    "print(f'LSTM Test Accuracy: {lstm_accuracy*100:.2f}%')\n",
    "print(f'LSTM F1 Score: {lstm_f1:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
