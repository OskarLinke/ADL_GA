{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    !pip3 install torch matplotlib torchmetrics scikit-image segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence # for padding data\n",
    "\n",
    "import pandas as pd # for making csv file\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import wandb\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the GPU\n"
     ]
    }
   ],
   "source": [
    "# GPU Support?\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using the GPU\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"Using the CPU\")\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataset of sequences containing characters 'a', 'b', and 'c' in order\n",
    "def gen_data(N=10000, min_len=3, max_len=20) -> list:\n",
    "    \"\"\"Generates a dataset of sequences containing characters 'a', 'b', and 'c' in order.\n",
    "    Parameters: N: int, number of sequences to generate\n",
    "                min_len: int, minimum length of a sequence\n",
    "                max_len: int, maximum length of a sequence\n",
    "    Returns: list of strings, each string is a sequence of characters 'a', 'b', and 'c' in order\"\"\"\n",
    "    dataset = []\n",
    "    max_member = max_len // 3 # three times this number is less than max_len\n",
    "    min_member = min_len // 3 # three times this number is less than min_len\n",
    "    if min_len < 3:\n",
    "        min_len = 3\n",
    "        print(\"Minimum length must be at least 3. Setting min_len to 3.\")\n",
    "    if max_len > 1000:\n",
    "        max_len = 1000\n",
    "        print(\"Maximum length must be at most 1000. Setting max_len to 1000.\")\n",
    "\n",
    "    for n in range(N):\n",
    "        if n % 4 == 0: # 25% of the time add actual member of formal language family\n",
    "            length = random.randint(min_member, max_member)\n",
    "            sequence = 'a' * length + 'b' * length + 'c' * length\n",
    "            dataset.append(sequence)\n",
    "        else:\n",
    "            length = random.randint(min_len-3, max_len-3)  # Random sequence length between 3 and (20-3 = 17). 3 is the smallest possible length for a sequence to be in the language\n",
    "            counts = [1, 1, 1] # Initialize counts for 'a', 'b', and 'c'. Will have at least one of each letter.\n",
    "\n",
    "            # Distribute the length among a, b, and c\n",
    "            for i in range(length):\n",
    "                counts[random.randint(0, 2)] += 1\n",
    "\n",
    "            # Ensure alphabetical order and create the sequence\n",
    "            sequence = 'a' * counts[0] + 'b' * counts[1] + 'c' * counts[2]\n",
    "            dataset.append(sequence)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for labels and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels\n",
    "def get_labels(data):\n",
    "    y = torch.zeros(len(data))\n",
    "    for i, sequence in enumerate(data):\n",
    "        if sequence == 'a'*(len(sequence)//3) + 'b'*(len(sequence)//3) + 'c'*(len(sequence)//3):\n",
    "            y[i] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a one-hot encoding of the sequences and a labels tensor\n",
    "def one_hot_encode(sequence):\n",
    "    encoded = torch.zeros(len(sequence), 3)\n",
    "    for i, char in enumerate(sequence):\n",
    "        encoded[i, 'abc'.index(char)] = 1\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]]) \n",
      " abc\n"
     ]
    }
   ],
   "source": [
    "# Example of one-hot encoding\n",
    "dataset = gen_data()\n",
    "encoded_dataset = [one_hot_encode(sequence) for sequence in dataset]\n",
    "print(encoded_dataset[0], \"\\n\", dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 20, 3])\n"
     ]
    }
   ],
   "source": [
    "# pad sequences\n",
    "padded_dataset = pad_sequence(encoded_dataset, batch_first=True)\n",
    "print(padded_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoded_dataset(N, min_len=3, max_len=20):\n",
    "    dataset = gen_data(N, min_len, max_len)\n",
    "    encoded_dataset = [one_hot_encode(sequence) for sequence in dataset]\n",
    "    padded_dataset = pad_sequence(encoded_dataset, batch_first=True)\n",
    "    y = get_labels(dataset)\n",
    "    return padded_dataset, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make training data\n",
    "train_size = int(0.8*len(dataset)) # 80/20 train/test split\n",
    "test_size = len(dataset) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8000, 20, 3]) torch.Size([2000, 20, 3])\n",
      "torch.Size([8000]) torch.Size([2000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data = padded_dataset[:train_size]\n",
    "test_data = padded_dataset[train_size:]\n",
    "y_train = get_labels(dataset[:train_size])\n",
    "y_test = get_labels(dataset[train_size:])\n",
    "print(train_data.shape, test_data.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3180)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.sum()/len(y_train) # % of sequences that are in the language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(list(zip(train_data, y_train)), batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(list(zip(test_data, y_test)), batch_size=test_data.size(0), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 20, 3]) torch.Size([128])\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]]) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# Test train_loader\n",
    "for data, labels in train_loader:\n",
    "    print(data.shape, labels.shape)\n",
    "    print(data[0], labels[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # x -> (batch_length, seq_length, input_size/vocab_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out, _ = self.rnn(input)\n",
    "        # out -> (batch_size, seq_length, hidden_size)\n",
    "        # reshape to get last output\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # x -> (batch_length, seq_length, input_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out, _ = self.lstm(input)\n",
    "        # out -> (batch_size, seq_length, hidden_size)\n",
    "        # reshape to get last output\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps and Sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make dataset with labels to csv\n",
    "# df = pd.DataFrame(dataset)\n",
    "# df['label'] = get_labels(dataset)\n",
    "# df.to_csv('formal_language.csv', index=False)\n",
    "# min_len, max_len = min(len(seq) for seq in dataset), max(len(seq) for seq in dataset)\n",
    "\n",
    "# # Instantiate a WandB run\n",
    "# wandb.login()\n",
    "# run = wandb.init(project=\"formal_language_rnn_lstm\")\n",
    "\n",
    "# # Create an artifact for data\n",
    "# artifact = wandb.Artifact(f\"FL_data_{str(min_len)}_{str(max_len)}\", type=\"dataset\") \n",
    "# artifact.add_file(\"formal_language.csv\") \n",
    "# run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sweeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep config\n",
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"loss\", \"goal\": \"minimize\"},\n",
    "    \"parameters\": {\n",
    "        \"model\": { \"values\": [\"RNN\", \"LSTM\"] },\n",
    "        \"epochs\": {\"values\": [200, 500] },    \n",
    "        \"optimizer\": { \"values\": [\"SGD\", \"Adam\"] },\n",
    "        \"hidden_size\": {\n",
    "            \"values\": [2, 20, 50]\n",
    "        },\n",
    "        \"num_layers\": {\n",
    "            \"values\": [1, 2]\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.001, 0.01]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    with wandb.init(project=\"formal_language_rnn_lstm\", config=config):\n",
    "        config = wandb.config\n",
    "\n",
    "        # Get hyperparameters\n",
    "        hidden_size = config.hidden_size\n",
    "        num_layers = config.num_layers\n",
    "        learning_rate = config.learning_rate\n",
    "        num_epochs = config.epochs\n",
    "\n",
    "        # Input size and number of classes\n",
    "        num_classes = 1 # binary classification\n",
    "        input_size = 3 # 'a' 'b' 'c'\n",
    "\n",
    "        # Set criterion\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Get model\n",
    "        if config.model == \"RNN\":\n",
    "            model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "        else:\n",
    "            model = LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "        \n",
    "        # Get optimizer\n",
    "        if config.optimizer == \"SGD\":\n",
    "            optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "        else:\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        wandb.watch(model, criterion, log=\"all\")\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            for data, labels in train_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                wandb.log({\"epoch\": epoch+1, \"loss\": loss.item()})\n",
    "            if (epoch+1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "        print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sweep_id\n",
    "# sweep_id = wandb.sweep(sweep_config, project=\"formal_language_rnn_lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the sweep\n",
    "# wandb.agent(sweep_id, function=train, count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train based on Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "input_size =  3 # 'a' 'b' 'c'\n",
    "num_classes = 1 # binary classification\n",
    "hidden_size = 40 # based on sweep\n",
    "num_layers = 1 # hyperparameter; can be tuned\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss() # \"hyperparameter\" (maybe BCE without LogitsLoss is better?)\n",
    "learning_rate = 0.005 # Between sweep rates (strongly uncorrelated with loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30], Loss: 0.1164\n",
      "Epoch [20/30], Loss: 0.1519\n",
      "Epoch [30/30], Loss: 0.3469\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop RNN\n",
    "num_epochs = 30 # slower convergence than LSTM\n",
    "for epoch in range(num_epochs):\n",
    "    for data, labels in train_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = rnn(data)\n",
    "        loss = criterion(outputs.squeeze(), labels) # BCEWithLogitsLoss expects 1D input, output from RNN is 2D\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30], Loss: 0.2475\n",
      "Epoch [20/30], Loss: 0.3070\n",
      "Epoch [30/30], Loss: 0.7901\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "lstm = LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=learning_rate) # Adam (no vanishing gradient problem)\n",
    "\n",
    "# Training loop LSTM\n",
    "num_epochs = 30 # convergence faster with Adam\n",
    "for epoch in range(num_epochs):\n",
    "    for data, labels in train_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = lstm(data)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_evaluation(model, loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data, label in loader:\n",
    "            data, label = data.to(device), label.to(device)\n",
    "\n",
    "            # Get the model's predictions\n",
    "            output = model(data)\n",
    "            pred = torch.round(torch.sigmoid(output)) # Sigmoid to get probabilities, round to get binary predictions\n",
    "\n",
    "            y_true.extend(label.tolist())\n",
    "            y_pred.extend(pred.tolist())\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report F1 Score (and accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Test Accuracy: 81.15%\n",
      "RNN F1 Score: 0.71\n",
      "LSTM Test Accuracy: 84.75%\n",
      "LSTM F1 Score: 0.79\n"
     ]
    }
   ],
   "source": [
    "rnn_accuracy, rnn_f1 = report_evaluation(rnn, test_loader)\n",
    "lstm_accuracy, lstm_f1 = report_evaluation(lstm, test_loader)\n",
    "\n",
    "\n",
    "print(f'RNN Test Accuracy: {rnn_accuracy*100:.2f}%')\n",
    "print(f'RNN F1 Score: {rnn_f1:.2f}')\n",
    "print(f'LSTM Test Accuracy: {lstm_accuracy*100:.2f}%')\n",
    "print(f'LSTM F1 Score: {lstm_f1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new test data of len 21 to 30 to see how well the models generalize\n",
    "long_data, y_long = get_encoded_dataset(1000, min_len=21, max_len=30)\n",
    "long_loader = DataLoader(list(zip(long_data, y_long)), batch_size=long_data.size(0), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_of_seq_len(model, loader):\n",
    "    model.eval()\n",
    "    length_to_true_pred = {}\n",
    "    with torch.no_grad():\n",
    "        for data, label in loader:\n",
    "            data, label = data.to(device), label.to(device)\n",
    "\n",
    "            # Get the model's predictions\n",
    "            output = model(data)\n",
    "            pred = torch.round(torch.sigmoid(output))\n",
    "            print(pred.sum().item())\n",
    "            \n",
    "            # Get sequence lengths\n",
    "            seq_lengths = (data.sum(dim=2) != 0).sum(dim=1).tolist()\n",
    "            for i, length in enumerate(seq_lengths):\n",
    "                if length not in length_to_true_pred:\n",
    "                    length_to_true_pred[length] = [[], []]\n",
    "                length_to_true_pred[length][0].append(label[i].item())\n",
    "                length_to_true_pred[length][1].append(pred[i].item())\n",
    "    f1_scores = {}\n",
    "    for length, (true, pred) in length_to_true_pred.items():\n",
    "        f1_scores[length] = f1_score(true, pred, average='binary', zero_division=0)\n",
    "    return f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.0\n",
      "461.0\n",
      "{24: 0.0, 29: 0.0, 25: 0.0, 27: 0.0, 30: 0.0, 21: 0.0, 26: 0.0, 22: 0.0, 28: 0.0, 23: 0.0}\n",
      "{24: 0.0, 29: 0.0, 25: 0.0, 27: 0.0, 30: 0.0, 21: 0.0, 26: 0.0, 22: 0.0, 28: 0.0, 23: 0.0}\n"
     ]
    }
   ],
   "source": [
    "rnn_f1 = f1_of_seq_len(rnn, long_loader)\n",
    "lstm_f1 = f1_of_seq_len(lstm, long_loader)\n",
    "print(rnn_f1)\n",
    "print(lstm_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
